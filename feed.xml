

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://c0ldstudy.github.io/</id>
  <title>c0ldstudy</title>
  <subtitle>A minimal, responsive and feature-rich Jekyll theme for technical writing.</subtitle>
  <updated>2025-03-24T07:03:01-07:00</updated>
  <author>
    <name>Jiacen (Jason) Xu</name>
    <uri>http://c0ldstudy.github.io/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://c0ldstudy.github.io/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://c0ldstudy.github.io/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator>
  <rights> Â© 2025 Jiacen (Jason) Xu </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>LLM Deploy</title>
    <link href="http://c0ldstudy.github.io/posts/LLM_Deploy/" rel="alternate" type="text/html" title="LLM Deploy" />
    <published>2023-11-10T00:00:00-08:00</published>
  
    <updated>2024-03-24T16:03:27-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/LLM_Deploy/</id>
    <content src="http://c0ldstudy.github.io/posts/LLM_Deploy/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
  

  
    <summary>
      





      In the blog, I would like to talk about my exploiting of Large Language Deployment in a linux server.


  TL; DR: I use LMDeploy to deploy the large language models like llama2 with the TurboMind backend and gradio frontend.


1.0 Preparation
Use the following command to install lmdeploy.

pip install lmdeploy

# install git lfs following the link (https://github.com/git-lfs/git-lfs/blob/main/I...
    </summary>
  

  </entry>

  
  <entry>
    <title>[SP2024] Understanding and Bridging the Gap Between Unsupervised Network Representation Learning and Security Analytics</title>
    <link href="http://c0ldstudy.github.io/posts/GSA/" rel="alternate" type="text/html" title="[SP2024] Understanding and Bridging the Gap Between Unsupervised Network Representation Learning and Security Analytics" />
    <published>2023-10-21T00:00:00-07:00</published>
  
    <updated>2023-11-20T09:51:07-08:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/GSA/</id>
    <content src="http://c0ldstudy.github.io/posts/GSA/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Papers" />
    
    <category term="Presentation" />
    
  

  
    <summary>
      





      Title: Understanding and Bridging the Gap Between Unsupervised Network Representation Learning and Security Analytics

  Author: Jiacen Xu, Xiaokui Shu and Zhou Li

  Conference: 2024 IEEE Symposium on Security and Privacy (SP)

  Paper: link

  Code: https://github.com/C0ldstudy/Argus

  Bibtex:
  @INPROCEEDINGS {,
author = {J. Xu and X. Shu and Z. Li},
booktitle = {2024 IEEE Symposium on Secu...
    </summary>
  

  </entry>

  
  <entry>
    <title>LLM Paper Summary</title>
    <link href="http://c0ldstudy.github.io/posts/LLM_Paper_Summary/" rel="alternate" type="text/html" title="LLM Paper Summary" />
    <published>2023-06-10T00:00:00-07:00</published>
  
    <updated>2023-06-10T00:00:00-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/LLM_Paper_Summary/</id>
    <content src="http://c0ldstudy.github.io/posts/LLM_Paper_Summary/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
    <category term="NLP" />
    
  

  
    <summary>
      





      1. Attention is All You Need
Paper/TF_code/Torch_code

Main Idea
The paper is the first to propose a parallel Transformer in the sequence transduction models.

Key insight
The problems of the RNN (LSTM, GRU) model: Need to calculate $h_t$ by $h_{t-1}$ which cannot be executed in parallel. While Transformer reduces the sequential computation

Model Structure:



  Position Encoding: they use sin...
    </summary>
  

  </entry>

  
  <entry>
    <title>Transformer Related Implementation</title>
    <link href="http://c0ldstudy.github.io/posts/Transformer_Related_Implementation/" rel="alternate" type="text/html" title="Transformer Related Implementation" />
    <published>2023-05-20T00:00:00-07:00</published>
  
    <updated>2023-05-20T00:00:00-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/Transformer_Related_Implementation/</id>
    <content src="http://c0ldstudy.github.io/posts/Transformer_Related_Implementation/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
    <category term="NLP" />
    
  

  
    <summary>
      





      1. Transformer[1]

I start it by following the video[1] from Andrej Karpathy and check the code from the repo. In the video, he shows how to build a GPT by oneself step by step which also shows the way to build a NLP model in a high level. I summarize these steps and add some useful tips that mentioned in the video.

1.1 Data Preprocessing
Tokenizer

1.2 Bigram Model
The code is here.
High-leve...
    </summary>
  

  </entry>

  
  <entry>
    <title>ChatGPT Prompt Course Note</title>
    <link href="http://c0ldstudy.github.io/posts/ChatGPT_Prompt/" rel="alternate" type="text/html" title="ChatGPT Prompt Course Note" />
    <published>2023-04-25T00:00:00-07:00</published>
  
    <updated>2023-05-08T21:20:38-07:00</updated>
  
    <id>http://c0ldstudy.github.io/posts/ChatGPT_Prompt/</id>
    <content src="http://c0ldstudy.github.io/posts/ChatGPT_Prompt/" />
    <author>
      <name>Jiacen (Jason) Xu</name>
    </author>

  
    
    <category term="Blogging" />
    
    <category term="NLP" />
    
  

  
    <summary>
      





      The note is for the course: DeepLearning.AI ChatGPT Prompt Engineering Course.

ChatGPT Prompt Engineering for Developers
Course link: https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction
1 Basic concepts:

  Base LLM: Predict the next word based on the training data.
  Instruction Tuned LLM: Fine-tune on instructions and good attempts at following instructions.


2 Prompting ...
    </summary>
  

  </entry>

</feed>


